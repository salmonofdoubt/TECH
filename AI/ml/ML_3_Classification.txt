##################################
   Misc Notes Online Course
##################################

############ PYTHON ###############
### check code      : cmd + i   ###
### run code        : cmd enter ###
### set working dir : Fn F5     ###
###################################

############### R ####################
### check code      : Fn F1        ###
### run code        : cmd enter    ###
### set working dir : files 'more' ###
######################################


##########################################
###     PART 3 - CLASSIFICATION        ###
##########################################


############################
###    INTRO & SUMMARY   ###
############################

- Unlike regression which predicts a continuous number, classification predicts a category. 
   - Regression:     predicting some real value
   - Classification: classifying, splitting a population linearly, curved, dimension elevated, decision trees.

- Classification applications range from medicine to marketing.

- Classification models include linear models like 

  - Logistic Regression
  - K-Nearest Neighbors (K-NN)
  - Support Vector Machine (SVM)
  - Kernel SVM
  - Naive Bayes
  - Decision Tree Classification
  - Random Forest Classification

-           learning
            /      \
           /        \
    supervised       \
   Learning from the k\nown label data to create a model, then predicting target class for the given input data.
   (basket of known fruit)
                        \
                         \
                    unsupervised
            Learning from the unlabeled data to just differentiating the given input data
            (basket of fruit with no idea what they are)

- Information Entropy: say before and after a Decision Tree split, the area is more homogenous and
  therefore has less Entropy. If it is fully homogenous, then the Entropy = 0. 

- Feature Scaling only ever necessary if there are Euclidian distance to be processed.

- Accuracy Raradox 
  - Happens when data is added but not classified 
  - Culminated Accuracy Profile counters this
    see Purchased vs Contacted curves, 
    curve get steeper with more promising people being contacted

         
its not spam,   its not spam,   
not detected    detected TYPE 1
      Predicted label 
           0    1
       0   TP | FP
Actual     35 |  5
label     ____|____
       1   FN | TN   
           10 | 50 
its spam,        its spam,
not detected     detected 
TYPE 2

Accuracy TP + TN = 85
Error    FP + FN = 15


############################
###  LOGISTIC REGRESSION ###
###     INTUITION        ###
############################

--------------------
What you now know: 
Logistic Regression is a linear classifier
In R, what is the function used to create a Logistic Regression classifier ? glm
Logistic Regression returns probabilities
In Python, what is the class used to create a logistic regression classifier ? LogisticRegression
In R, what value do we need to input for the family parameter ? binomial
--------------------

- Uses Linear Regression and linear classifiers
  a) for training set
  b) for test set
- Prediction Boundery is a straight linear line 
 _____________
| x o \  ox  |
| o xx \ o   |
|__x____\___o|


- Confusion Matrix: Number of correct and incorrect predictions
- Describes the performance of classification model

         
its not spam,   its not spam,   
not detected    detected TYPE 1
      Predicted label 
           0    1
       0   TP | FP
Actual     35 |  5
label     ____|____
       1   FN | TN   
           10 | 50 
its spam,        its spam,
not detected     detected 
TYPE 2

Accuracy TP + TN = 85
Error    FP + FN = 15

e.g:
- Was an action taken?
- Lin regression makes some sense
- Yes Probability higher with age 

Y/N?
 |
1|        x x  x xxxxx x                          _______
 |                                              /
0|xxxx x xx  x   x  x                  _______ /   
 |______________________AGE               y = b0 + b1*x (for the slope)

 - Thats the basis of the (smoother) Sigmoid function:

 y = b0 + b1*x
 p = 1/(1+e^-y)
 ln(p/(1-p) = b0 + b1 + b1*x

 - This is the logistic regression function
   - predicts probabilities p^ usually at +/- 50%


###############################
###      K-NN INTUITION     ###
###  K NEAREST NEIGHBORS    ###
###############################

What you know 
- In the K-NN algorithm we do need to specify the number of neighbours
- K-NN is a linear classifier
- K-NN prediction boundary usually doesnt look like a smooth curve
- In Python, KNeighborsClassifier is the class used to create a K-NN classifier
- 5 is the default parameter for the number of neighbours k

Notes
- A better, since non-linear, seperation between the areas
- K-NN puts each datapoint either in the x or o category
- Nearest Neighbour

y
 |       o  o  o
 |        n  o o oo  o                         
 |  x    x       oo  o                      
 |x x x x
 | x x x  x   x  x                    
 |______________________x 

 Cat x 
 Cat o      

- Also, if new data comes in, is n (new) x or rather o? 
- Euclidian distance D = ( (x2 - x1)^2 + (y2 - y1)^2 )^0.5


###############################
###      SVM INTUITION      ###
### SUPPORT VECTOR MACHINES ###
###############################

- How to draw the best line between the categories
- Again the distance of the points 
- Actually only the two supporting points are considered
- Extend perpendicular 
- Seen from 0,0 they are vectors

- Bottom left is very x-like, top right is very y-like, away from support vector
- These far away regions are the best samples to learn from.  

y
 |       o  o  o  
 |         (o)  o oo  o                         
 |  x    (x)     oo  o                      
 |x x x x
 | x x x  x   x  x                    
 |______________________x 


###############################
###   KERNEL SVM INTUITION  ###
###    KERNAL SUPPORT       ###
###    VECTOR MACHINES      ###
###############################

- Kernel the center of the elevated additional dimension
- http://mlkernels.readthedocs.io/en/latest/kernels.html
- What if SVM alone cannot make a decent lineary boundary
- e.g. only non-lineary seperation?
- Lets map to another higher dimension, now it becomes possible
- Computationally intense


1D - not linearly seperable

| xxx x   o oo  x x x  x
0 _______________________10 x1 


f = x -5 

| xxx x   o oo  x x x  x
 ______0______________________10 x1 


f = (x -5)^2 (parable) 
2D awesome, now linearly seperable

                  x  /              
| x               x/
   x           x / 
    x          / o
      x      /oo  
 ______0___/__________________10 x1 
        /

3D Seperation for other problems, line becomes a hyperplane

- Analyse
- Convert back to original dimension


* Gaussion RBF Kernel
- on a 2D scatter plot, apply 
- This becomes a 3D model

K(x,l) = e^-((x - l)^2 / 2sigma^2)

- Landmark, is right in the middle, highest
- further away from Landmark the function of coord x,l will be very small  

- So the kernel func is a circle, moving it up and down
  the 3D Gaussian curve (depending on sigma), a category seperation is possible

* Sigmoid Kernel

* Polynomial Kernel


###############################
###     BAYES THEOREM       ###
###############################

P(A|B) = P(B|A) * P(A) / P(B)

Intuition
----------------------
1000 spanners 
400 from M2
1% have a defect = 10
50% defects from M2 = 5
5/400 = 1.25%

More exact calculation
----------------------
- Spanners produced by Machine 1 and 2
  
  M1 30/hr > 30/50(tot), prob that piece (defect or not) came from M1
  P(M1) = 0.6 
  
  M2 20/hr > 20/50(tot)
  P(M2) = 0.4

- 1% of all of them is defect
  
  P(defect) = 1%

- 50% of all defective came from M1, 50% from M2
  
  P(M1|Defect) = 0.5
  P(M2|Defect) = 0.5

- Whats the probability a spanner made by M2 is defect? 
 
 So, the general equation 
 P(A|B) = P(B|A) * P(A) / P(B)

 with the givens
  P(M2) = 0.4
  P(defect) = 1%
  P(M2|Defect) = 0.5
  P(Defect|M2) ???

will look like this:
P(Defect|M2) = P(M2|Defect) * P(Defect) / P(M2)
             = 0.5          * 0.01      / 0.4
             = 0.0125 (1.25% of defects come from M2)

P(Defect|M1) = P(M1|Defect) * P(Defect) / P(M1)
             = 0.5          * 0.01      / 0.6
             = 0.083 (0.83% of defects come from M1)


- obviously 0.5% defective from M2... 


######################################
###   NA√èVE BAYES CLASSIFICATION   ###
######################################

- Probabilistic type of classifcation
- Independence assumptions (salary - age not related but they are)
- If only 2 classes, second can be derived c1 + c2 = 1.0

 ______________________________________________
| posterior   likelihood * prior  / marginal   |
| prob      =              prob   / likelihood | 
| P(A|B)    = P(B|A)     * P(A)   / P(B)       |
|______________________________________________|

- How to apply this to ML?

age
 |       d  d  d  
 |         d  d dd  d                         
 |  w    w (n)   dd  d                      
 |w w w w 
 | w  ww   ww w                     
 |______________________salary 

w walks
o drives
n - what will they do?

- Note: this is supervised learning (categories, or labels, alredy exist)  

- POA:
 - Apply Naive Bayes Theorem to new point twice:
   P(A|B)     = P(B|A)      * P(A)      / P(B) 
   for walkers and drivers

   P(Walks|X)  = P(X|Walks)  * P(Walks) / P(X)
   1. Prior Prob:
        P(Walks) = #Walkers / tot_obs  ; say 10/30
   2. Marginal likelihood
        P(X) #similar_obs / tot_obs    ; any points in Circle(r) excl new point, say 4/30
                                       ; Likelihood any new point falls into that circle
   3. Likelihood (that someone walks)
        P(X|walks) = #walkers circ /   ; likelihood of walkers from circ to all walkers,
                      #tot_walkers     ; say 3/10

   P(Walks|X) = 0.75


   P(Drives|X) = P(X|Drives) * P(Drives) / P(X)

   1. Prior Prob:
        P(Drives) = #Drivers / tot_obs ; say 20/30
   2. Marginal likelihood
        P(X) #similar_obs / tot_obs    ; Any points in Circle(r) excl new point, say 4/30
                                       ; Likelihood any new point falls into that circle
   3. Likelihood (that someone walks)
        P(X|drives) = #drivers circ /  ; likelihood of drivers from circ to all drivers,
                      #tot_drivers     ; say 1/20

   P(Drives|X) =  1/20 * 20/30 / 4/30 = 0.25


 - Compare P(Walks|X) vs P(Drives|X) and from there decide with class
   a datapoint should go P(Walks|X) = 0.75  <>  P(Drives|X) = 0.25  (SUM = 1.0) 
 - Hence, this is someone with 0.75 chance to Walk

 - Walker / Drivers have features age, and salary, but we decide on probability


##################################
###  Decision Tree Intuition   ###
##################################

- prediction boundary will be rectangular shapes
- Will try to accommodate every datapoint, in Python it seems overfitting takes place 
- see also CART Classification and Regression Trees

      x2 < value
        /  \
      yes  no
    x1<70  x1<50  
    /  \    /  \
  yes  no  yes  no
  next layer of conditions 
/  \  /  \  /  \   /  \
                  yes  no

- Trees are simple, also very old
- In combination with other methods they can be popular still


##################################
###  Random Forest Intuition   ###
##################################

- Ensemble Learning: leverages many, same or not, methods
- Overfitting may still be an issue
- The power of numbers
  1. Pick random K data points from Training set 
  2. Build decsion Tree associated to these K data points
  3. Choose # Ntree of trees you want to build, repeat 1&2
  4. For a new data point make each one of your NTree trees predict the category, 
     assign new data point to category that wins majority vote

- Applied here 
  https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/BodyPartRecognition.pdf














