##################################
   Misc Notes Online Course
##################################

############ PYTHON ###############
### check code      : cmd + i   ###
### run code        : cmd enter ###
### set working dir : Fn F5     ###
###################################

############### R ####################
### check code      : Fn F1        ###
### run code        : cmd enter    ###
### set working dir : files 'more' ###
######################################


###########################################
### PART 7 Natural Language Processing  ###
###########################################


############################
###    INTRO & SUMMARY   ###
############################

Natural Language Processing (or NLP) is applying Machine Learning models to text and language. Teaching machines to understand what is said in spoken and written word is the focus of Natural Language Processing. Whenever you dictate something into your iPhone / Android device that is then converted to text, thatâ€™s an NLP algorithm in action.

You can also use NLP on a text review to predict if the review is a good one or a bad one. You can use NLP on an article to predict some categories of the articles you are trying to segment. You can use NLP on a book to predict the genre of the book. And it can go further, you can use NLP to build a machine translator or a speech recognition system, and in that last example you use classification algorithms to classify language. Speaking of classification algorithms, most of NLP algorithms are classification models, and they include Logistic Regression, Naive Bayes, CART which is a model based on decision trees, Maximum Entropy again related to Decision Trees, Hidden Markov Models which are models based on Markov processes.

A very well-known model in NLP is the Bag of Words model. It is a model used to preprocess the texts to classify before fitting the classification algorithms on the observations containing the texts.

In this part, you will understand and learn how to:

Clean texts to prepare them for the Machine Learning models,
Create a Bag of Words model
  - exclude the, an, numbers, 
  - stemming loved, loving = love
  - tokenization 
  - sparse matrix

Apply Machine Learning models onto this Bag of Worlds model.
  - Creating the Bag of Words model
    - After preprocessed "review" string, create cols for each word, and mark 1 of that word occurs in a string
    - sparse matrix contains a lot of 0s 
    - Classification based on 1 like or 0 dont like


########################
###  NLP_NAIVEBAYES  ###
########################

- Predict category of a text 
- Using Naive Bayes 

cm was 
[[55 42]
 [12 91]]

 its not spam,   its not spam,   
not detected    detected TYPE 1
      Predicted label 
           0    1
       0   TP | FP
Actual     35 |  5
label     ____|____
       1   FN | TN   
           10 | 50 
its spam,        its spam,
not detected     detected 

- Trying some other classifiaction algorithms

def printall():
  TP = 72
  TN = 64
  FP = 25
  FN = 39
  Accuracy  = (TP + TN) / (TP + TN + FP + FN)
  Precision = TP / (TP + FP)
  Recall    = TP / (TP + FN)
  F1Score  = 2 * Precision * Recall / (Precision + Recall)
  print("NLP_CART-ENTROPY")
  print(cm)
  print("Accuracy", Accuracy)
  print("Precision", Precision)
  print("Recall", Recall)
  print("F1Score", F1Score)

printall()



>>> printall()
NLP_NAIVEBAYES
[[55 42]
 [12 91]]
Accuracy 0.73
Precision 0.5670103092783505
Recall 0.8208955223880597
F1Score 0.6707317073170731

>>> printall()
NLP_LOGISTICREGRESSION
[[76 21]
 [37 66]]
Accuracy 0.71
Precision 0.7835051546391752
Recall 0.672566371681416
F1Score 0.7238095238095238

>>> printall()
NLP_K-NN
[[74 23]
 [55 48]]
Accuracy 0.61
Precision 0.7628865979381443
Recall 0.5736434108527132
F1Score 0.654867256637168

>>> printall()
NLP_SVM
[[74 23]
 [33 70]]
Accuracy 0.72
Precision 0.7628865979381443
Recall 0.6915887850467289
F1Score 0.7254901960784315

>>> printall()
NLP_KERNEL-SVM-RBF
[[ 97   0]
 [103   0]]
Accuracy 0.485
Precision 1.0
Recall 0.485
F1Score 0.6531986531986532

>>> printall()
NLP_DECISION-TREE
[[74 23]
 [35 68]]
Accuracy 0.71
Precision 0.7628865979381443
Recall 0.6788990825688074
F1Score 0.7184466019417477

>>> printall()
NLP_DECISION-TREE
[[85 12]
 [47 56]]
Accuracy 0.705
Precision 0.8762886597938144
Recall 0.6439393939393939
F1Score 0.74235807860262

>>>>>>>>>>>>>>>>>>
>>> Additional >>>
>>>>>>>>>>>>>>>>>>
>>> printall()
NLP_CART
[[68 29]
 [42 61]]
Accuracy 0.645
Precision 0.7010309278350515
Recall 0.6181818181818182
F1Score 0.6570048309178743

>>> printall()
NLP_CART-GINI
[[72 25]
 [43 60]]
Accuracy 0.66
Precision 0.7422680412371134
Recall 0.6260869565217392
F1Score 0.6792452830188681

>>> printall()
NLP_CART-ENTROPY
[[72 25]
 [39 64]]
Accuracy 0.68
Precision 0.7422680412371134
Recall 0.6486486486486487
F1Score 0.6923076923076923




>>> 


 HWK
1. Run the other classification models we made in Part 3 - Classification, other than the one we used in the last tutorial.
2. Evaluate the performance of each of these models. Try to beat the Accuracy obtained in the tutorial. But remember, Accuracy is not enough, so you should also look at other performance metrics like Precision (measuring exactness), Recall (measuring completeness) and the F1 Score (compromise between Precision and Recall). Please find below these metrics formulas (TP = # True Positives, TN = # True Negatives, FP = # False Positives, FN = # False Negatives):

Accuracy = (TP + TN) / (TP + TN + FP + FN)
Precision = TP / (TP + FP)
Recall = TP / (TP + FN)
F1 Score = 2 * Precision * Recall / (Precision + Recall)

3. Try even other classification models that we havent covered in Part 3 - Classification. Good ones for NLP include:
CART
C5.0
Maximum Entropy
Submit your results in the Q&A for this Lecture or by pm and justify in few words why you think its the most appropriate model.





