##################################
   Misc Notes Online Course
##################################

############ PYTHON ###############
### check code      : cmd + i   ###
### run code        : cmd enter ###
### set working dir : Fn F5     ###
###################################

############### R ####################
### check code      : Fn F1        ###
### run code        : cmd enter    ###
### set working dir : files 'more' ###
######################################


#############################################
###  PART 10  Model Selection & Boosting  ###
#############################################


############################
###    INTRO & SUMMARY   ###
############################

After we built our Machine Learning models, some questions remained unanswered:

How to deal with the bias variance tradeoff when building a model and evaluating its performance ?
How to choose the optimal values for the hyperparameters (the parameters that are not learned) ?
How to find the most appropriate Machine Learning model for my business problem ?
In this part we will answer these questions thanks to Model Selection techniques including:

k-Fold Cross Validation
Grid Search
Eventually we will finish this course by a last bonus section included in this part, dedicated to one of the most powerful Machine Learning model, that has become more and more popular: XGBoost.


####################
###    XGBoost   ###
####################

Kernel parameters: find best, or hyper parameters
k-fold cross validation
