##################################
   Misc Notes Online Course
##################################

############ PYTHON ###############
### check code      : cmd + i   ###
### run code        : cmd enter ###
### set working dir : Fn F5     ###
###################################

############### R ####################
### check code      : Fn F1        ###
### run code        : cmd enter    ###
### set working dir : files 'more' ###
######################################


##########################################
###        PART 4 - CLUSTERING         ###
##########################################


############################
###    INTRO & SUMMARY   ###
############################

		    learning
            /      \
           /        \
    supervised       \
   Learning from the k\nown label data to create a model, then predicting target class for the given input data.
   (basket of known fruit)
                        \
                         \
                    unsupervised
            Learning from the unlabeled data to just differentiating the given input data
            (basket of fruit with no idea what they are)


Clustering is similar to classification, but the basis is different. In Clustering you donâ€™t know what you are looking for, and you are trying to identify some segments or clusters in your data. When you use clustering algorithms on your dataset, unexpected things can suddenly pop up like structures, clusters and groupings you would have never thought of otherwise.

In this part, you will understand and learn how to implement the following Machine Learning Clustering models:

K-Means Clustering
Hierarchical Clustering


---Quizz---
* Dendrograms are the method to use in Hierarchical Clustering to find the right number of clusters
* Within-cluster variance is the metric dendrograms are used
* Hierarchical Clustering does not perform better than K-Means on large datasets
* In Python, AgglomerativeClustering class is used to fit hierarchical clustering to a dataset
* In R, hclust function can be used to fit hierarchical clustering to a dataset




############################
###       K-MEANS        ###
###       K-MEANS++      ###
############################

- Iterative process 

1. Choose number K of clusters
2. Select random K points (centroids C, not necessarily from dataset)
3. Assign each datapoint to closest centroid > forms K clusters
4. Compute centre of mass and place (move) new centroid of each cluster
5. Convergence: Reassign each datapoint to new closest centroid. 
   If reassignment took place goto 4, esle FIN. 

y
 |       x  x  x  C
 |        x  x x xx  x                         
 |  x  C  x       xx  x                      
 |x x x x
 | x x x  x   x  x                    
 |______________________x 

- Euclidian distance or other type of distance? Up to you..

- Chosing initial centroid could be a trap
  - Modification: K-Means++ algo

- What is the right number of clusters?
  - WCSS = SUM(distance(PiCx)^2)  | WCSS decreases with # of clusters, once #clusters == #datapoints WCSS = 0
           1->x
    drops towards 0 exponentially, chose elbow point after which gain isnt great anymore 

- Quiz
In the K-Means algorithm, we do have to specify the number of clusters.
WCSS metric can be used to find an optimal number of clusters
We cannot choose any random initial centroids at the beginning of K-Means.
In Python, K-means++ is the recommended init parameter to input
In R, clusplot is a good function to plot clusters
 

##############################
### Hierachical Clustering ###
###      HC Intuition      ###
##############################

Agglomerative & Divisive
1. Each datapoint is a cluster
2. Take the 2 closest data points and make them one cluster 
    N-1
3. Take 2 closest cluster (possible still single datapoints) and make them one cluster     
    N-2
	Need to be specific here: Center of mass of cluster, or?
4. Repeat 3. > FIN

Agglomerative retains a memory of clusters in a Dendrogram
- Based on least dissimilarity of 2 points

                                 Euclidian 
                                 distance  .    ___________     .
y|                p3                    |  .   |           |    .
 |           p2                       ..|..v...|...........|....v.... Threshold for the 2 clusters
 |      p4       p1                     |      |         __|__
 | p5                                   |    __|__      |    _|_
 |     p6                               |   |    _|_    |   |   |
 |______________________x               |___|___|___|___|___|___|___ 
                                           p1  p2  p3  p4  p5  p6 

- to find the threshold for best # of clusters, just take largest vertical distance,
  move down furthest without crossing a horizontal line (across the dendrogram) 






