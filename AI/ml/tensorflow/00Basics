TensorFlow Programming Concepts
  tensors
  operations
  graphs
  sessions

Build a simple TensorFlow program that creates a default graph, and a session that runs the graph

Overview of Concepts
----------------------
TensorFlow gets its name from tensors, which are arrays of arbitrary dimensionality.
Using TensorFlow, you can work with tensors having a very high number of dimensions. 
That said, much of your work will likely take place in the following more mundane dimensions:
A scalar is a 0-d array (a 0th-order tensor). For example, "Howdy" or 5
A vector is a 1-d array (a 1st-order tensor). For example, [2, 3, 5, 7, 11] or [5]
A matrix is a 2-d array (a 2nd-order tensor). For example, [[3.1, 8.2, 5.9][4.3, -2.7, 6.5]]

TensorFlow operations create, destroy, and manipulate tensors. 
Most of the lines of code in a typical TensorFlow program are operations.
A TensorFlow graph (also known as a computational graph or a dataflow graph) is, yes, a graph data structure.
Many TensorFlow programs consist of a single graph, but TensorFlow programs may optionally create multiple graphs.
A graph's nodes are operations; a graph's edges are tensors. Tensors flow through the graph, 
manipulated at each node by an operation. 
The output tensor of one operation often becomes the input tensor to a subsequent operation. 
TensorFlow implements a lazy execution model, meaning that nodes are only computed when needed,
based on the needs of associated nodes.
TensorFlow provides a utility named TensorBoard to help you visualize the graph.

A x-------x B   nodes are operations
  |       |     lines are tensors, multidim arrays
  |       |
  |       |
C x-------x--------x 
           D        E

Tensors can be stored in the graph as constants or variables. 
As you might guess, constants hold tensors whose values can't change, while variables hold tensors whose values can change. 
However, what you may not have guessed is that constants and variables are just more operations in the graph. 
A constant is an operation that always returns the same tensor value. 
A variable is an operation that will return whichever tensor it has been assigned to.
To define a constant use the tf.constant operator and pass in its value. For example:
  x = tf.constant([5.2])
Similarly, you can create a variable like this:
  y = tf.Variable([5])
Or you can create the variable first and assign a value later like this (note that you always have to specify a default value):
  y = tf.Variable([0])
  y = y.assign([5])
  
Once you've defined some constants or variables, you can combine them together with other operations like tf.add.
When you evaluate the tf.add operation, it will call your tf.constant or tf.Variable operations to get their values and
then return a new tensor with the sum of those values.

An important difference with using tf.Variables is that you must explicitly initialize them by creating a
tf.global_variables_initializer operation and calling it at the start of your session.

Graphs must run within a TensorFlow session. 

A session holds all state for the graph(s) it runs. Furthermore, a session can distribute graph execution across multiple machines (assuming the program is run on Borg or some other distributed computation framework).
So, TensorFlow programming essentially involves:
Assembling constants, variables, and operations into a graph.
Evaluating those constants, variables and operations within a session.
