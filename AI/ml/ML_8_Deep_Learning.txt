##################################
   Misc Notes Online Course
##################################

############ PYTHON ###############
### check code      : cmd + i   ###
### run code        : cmd enter ###
### set working dir : Fn F5     ###
###################################

############### R ####################
### check code      : Fn F1        ###
### run code        : cmd enter    ###
### set working dir : files 'more' ###
######################################


###########################################
###     PART 8 DEEP LEARNING            ###
###########################################


############################
###    INTRO & SUMMARY   ###
############################

Deep Learning is the most exciting and powerful branch of Machine Learning. Deep Learning models can be used for a variety of complex tasks:

* Artificial Neural Networks for Regression and Classification

* Convolutional Neural Networks for Computer Vision

* Recurrent Neural Networks for Time Series Analysis

* Deep Boltzmann Machines for Recommendation Systems

In this part, you will understand and learn how to implement the following Deep Learning models:

* Artificial Neural Networks for a Business Problem
* Convolutional Neural Networks for a Computer Vision task
* Any NN can be initialised via sequence of layers, or graph


#########################
### ARTIFICIAL NEURAL ###
###      NETWORKS     ###
#########################

The Issue
--------------
- Binary outcome (classification problem)
- "Deep": High power consuming problems
- Bank customers are leaving at unusual rates, understand why.
- Take 10000 snapshot of dataset, 6 months later who's left? 0 - left 1 - stays
- Who's is at risk of leaving, provide insights

Neurons
---------------

Input
values
X1              _
   \ W1       /   \
X2 - W2  --> |     | --> Output 
   / W3       \ _ /      value y
X3           SUM(WiXi) 
  
  Rectifier          Sigmoid 
  Func               Func   
                              _______
|       /                    /
|      /            _______ /   
|_____/_____        y = b0 + b1*x (for the slope)
                    probabilities


- Basically ranking of customers in terms of leaving

ANN with Stochastic Gradient Descend
------------------------------------
1. Randomly initilise weights to small numbers close to 0, but not 0
2. Input first observation of dataset (customer1) into input layer
   each feature in one input node
3. Fwd propagation from left to right: Neurons are activated
   Their activation depends on their Weight.
   Propagate activations until getting predicted result y
4. Compare predicted result to actual result. Measure generated error.
5. Back propagation from right to left: Update the weights according to
   how much they respond to error. Learning rate decides the update of weights.
6. Repeat 1..5 and update weights after each observation 
   (reinforcement learning)
   or
   Repeat 1..5 but update weights after batch observation 
   (batch learning)
7. Once whole training passed through ANN, that's an epoch. Redo more epochs. 


Epoch 100/100
8000/8000 [==============================] - 3s - loss: 0.4001 - acc: 0.8362     
[[1547   48]
 [ 271  134]]



Technicalities
---------------
Theano: calculation (using also GPU, more cores, more float computations)

Tensorflow: Google, also for GPU and deep learning

source activate tensorflow
source deactivate

Keras: Google, once both above are installed, install Keras, which will wrap both in just a few lines of code

from keras.models import Sequential     #initialize NN
from keras.layers import Convolution2D  #2D images: 1st step add convolutional layers
from keras.layers import MaxPooling2D   #2D images: 2nd step poolinglayers
from keras.layers import Flatten        #convert pool maps into vector
from keras.layers import Dense          #add full layers into classic ANN



#########################
###   CONVOLUTIONAL   ###
###  NEURAL NETWORKS  ###
#########################

- Medicine
- Image interpretation
- Pixels distribution - so there is no depend vars
- Keras helps, structure pics and place in folders
- sequence of layers 

* Convolution step:

0 0 0 1 0 0 0 1 1                
0 1 1|0 1 0|0 0 1     010       2 0 2 0 0 0 0
1 0 1|0 1 0|0 0 0   X 100  -->  3 0 0 0 4 0 0
1 1 1|0 0 0|1 0 1     110       3 3 2 2 0 1 1
1 1 0 1 0 1 0 0 1               

pic                 sliding     Resulting
                    feature     Feature
                    detector    map 

* Pooling is similar, reducing feature maps
  Reduces neural nodes 
20202232          
32323133   01     2632
11312122   10     5212
13312323          1422  
                  
                  Pooled 
                  Feature
                  Maps

* Flattening

2632          
5212    -->  2  6  3  2  5  2  1  2  1  4  2  2
1422         X1 X2 X3
             1 single vector of all feature maps (input)

* Comparing pix by pix would be wrong, you want to know how they relate to neighbors

* Image Augmentation > if too few images, create batches of images so to avoid overfitting

* Epoch 25/25
8000/8000 [==============================] - 142s - loss: 0.2527 - acc: 0.8936 - val_loss: 0.4698 - val_acc: 0.8180
(TRAINING_SET)                            (TEST_SET)

Say if TEST_SET is below of 80%, there is room for improvement to increase TEST_SET accuracy
* make it deeper: add a convolutional layer, or a fully connected layer

